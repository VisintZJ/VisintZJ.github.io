---
title: 向量、矩阵、机器学习中的范数
categories:
  - blog
tags:
  - 数学
  - 范数
  - 机器学习
excerpt: 向量、矩阵、机器学习都有范数的概念，他们的计算方法各不相同，容易混淆。
usemathjax: true
---
# 范数
阅读论文时，经常会用到向量或矩阵的范数表达式，而因为向量范数与矩阵范数不同的计算公式，经常会造成混淆，因此记录下来以备后续的查阅。

## 向量的范数
**向量的1-范数：** $ \lVert X \rVert_1 = \sum_{i=1}^n\lvert x_i\rvert $  
指各个元素的绝对值之和。  
**向量的2-范数：** $\lVert X \rVert_2 = (\sum_{i=1}^n x_i^2)^{\frac{1}{2}} = \sqrt{\sum_{i=1}^n x_i^2}$  
指每个元素的平方和再开平方。  
**向量的无穷范数：** $\lVert X \rVert_\infty = \max\limits_{1\leq i\leq n} \lvert x_i \rvert$  
指向量中元素绝对值中最大的。  
**向量的p-范数：** $\lVert X \rVert_p = (\sum_{i=1}^n \lvert x_i \rvert^p)^\frac{1}{p}$  
其中正整数$p\geq1$，并且有$\lim\limits_{p\rightarrow\infty}\lVert X \rVert_p = \max\limits_{1\leq i\leq n} \lvert x_i \rvert$。

## 矩阵的范数
虽然矩阵也有1-范数、2-范数，但是他们的公式并不相同。  
**矩阵的1-范数（列模）：**  $\lVert A \rVert_1 = \max\limits_{X\neq0}\frac{\lVert AX \rVert_1}{\Vert X \rVert_1} = \max\limits_{1\leq j \leq n}\sum\limits_{i=1}^{n}\lvert a_{ij} \rvert$  
矩阵的每一列上的元素绝对值现求和，再从中取最大的列和。  
**矩阵的2-范数（谱模）：** $\lVert A \rVert_2 = \max\limits_{X\neq 0}\frac{\lVert AX\rVert_2}{\lVert X\rVert_2} = \sqrt{\lambda_{max}(A^{T}A)} = \sqrt{\max\limits_{1\leq i\leq n}\lvert\lambda_i\rvert}$  
其中，$\lambda_i$为$A^{T}A$的特征值，最后结果为矩阵$A^{T}A$的最大特征值开平方。  
**矩阵的无穷范数（行模）：** $\lVert A\rVert_\infty = \max\limits_{X\neq 0}\frac{\lVert AX\rVert_\infty}{\lVert X \rVert_\infty} = \max\limits_{1\leq i\leq n}\sum_{j=1}^n\lvert a_{ij}\rvert$  
矩阵的每一行上的元素绝对值先求和，再从中取最大的绝对值和。

## 机器学习中的范数
多与稀疏表示知识相关。  
**矩阵的核范数：**矩阵的奇异值（将矩阵svd分解）之和，这范数可以用来低秩表示（因为最小化核范数，相当于最小化矩阵的秩——低秩）；  
**矩阵的L0范数：**矩阵的非0元素的个数，通常用它来表示稀疏，L0范数越小0元素越多，也就越稀疏；  
**矩阵的L1范数：**矩阵中的每个元素的绝对值之和，他是L0范数的最优凸近似，因此它也可以近似表示稀疏；  
**矩阵的F范数：**矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的**L2范数**，它的优点是它是一个凸函数，可以求导求解，易于计算；  
**矩阵的L21范数：**矩阵先以每一列为单位，求每一列的F范数（也可认为是向量的2范数），然后再将得到的结果求L1范数（也可认为是向量的1范数），很容易看出它是介于L1和L2之间的一种范数。
